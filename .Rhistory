lda.class = lda.pred$class
# In R, we can use a boolean operator to determine accuracy.  Consider the next line of code.
match = lda.class == test_set$Type
# Determine the percent of accuracy using match.
match
q = 0
for(i in match)
{
if (i=="TRUE"){
q = q + 1
}
}
accuracy = q / length(match) * 100
accuracy
#
# Title: Glass Classification Project
#
# Author: Amanda Landi
#
# Sources:
# (1) where the data comes from: package mlbench, see documentation for details
#
# You need to install the following package
# You do not need to do so every time.  Only once.
# You may comment out the next two lines after you run them the first time.
# install.packages("mlbench")
# install.packages("MASS")
# However, you will need to run the next three lines of code
# Everytime you reopen the script.
library(mlbench)
library(MASS)
data("Glass")
# replace the underline with a function that allows you
# to see the structure of the data.
# Q: Describe the data in your report.
# RSC: str() function gets structure of data
str(Glass)
# Q: How large is your sample?
n = dim(Glass)
# Q: What percent of your data will become your training data? Why?
# Q: If we have too high a percentage, what do we need to worry about?
p = .9
# Please mimick what we did in the spam project when we extracted p% of the
# original data to creating a training data
percent = floor(n[1]*p)
t = sample(1:n[1], percent, replace = FALSE)
ind_train = t
test_set = Glass[-t,]
# Use the help page (or online documentation that you cite) to determine
# the appropriate use of the lda function
# Please choose only two of independent variables (the columns).
lda.fit = lda(Glass$Type ~ ., data = Glass[,c(1,2)], subset = t)
# the next line of code allows you to view your results
# Q: Discuss the prior probability results.
# Q: Discuss the meaning of Group Means.
# Q: What information do the coefficients give you?
lda.fit
# Create a graph.  You should use the help page to figure out how to
# include a meaningful title and axis labels.
plot(lda.fit, col = lda.fit$counts)
# We want to predict the class for our test set.  Recall how we used the
# predict function in the last project.
lda.pred = predict(lda.fit, test_set)
lda.class = lda.pred$class
# In R, we can use a boolean operator to determine accuracy.  Consider the next line of code.
match = lda.class == test_set$Type
# Determine the percent of accuracy using match.
match
q = 0
for(i in match)
{
if (i=="TRUE"){
q = q + 1
}
}
accuracy = q / length(match) * 100
accuracy
#
# Title: Glass Classification Project
#
# Author: Amanda Landi
#
# Sources:
# (1) where the data comes from: package mlbench, see documentation for details
#
# You need to install the following package
# You do not need to do so every time.  Only once.
# You may comment out the next two lines after you run them the first time.
# install.packages("mlbench")
# install.packages("MASS")
# However, you will need to run the next three lines of code
# Everytime you reopen the script.
library(mlbench)
library(MASS)
data("Glass")
# replace the underline with a function that allows you
# to see the structure of the data.
# Q: Describe the data in your report.
# RSC: str() function gets structure of data
str(Glass)
# Q: How large is your sample?
n = dim(Glass)
# Q: What percent of your data will become your training data? Why?
# Q: If we have too high a percentage, what do we need to worry about?
p = .9
# Please mimick what we did in the spam project when we extracted p% of the
# original data to creating a training data
percent = floor(n[1]*p)
t = sample(1:n[1], percent, replace = FALSE)
ind_train = t
test_set = Glass[-t,]
# Use the help page (or online documentation that you cite) to determine
# the appropriate use of the lda function
# Please choose only two of independent variables (the columns).
lda.fit = lda(Glass$Type ~ ., data = Glass[,c(1,2)], subset = t)
# the next line of code allows you to view your results
# Q: Discuss the prior probability results.
# Q: Discuss the meaning of Group Means.
# Q: What information do the coefficients give you?
lda.fit
# Create a graph.  You should use the help page to figure out how to
# include a meaningful title and axis labels.
plot(lda.fit, col = lda.fit$counts)
# We want to predict the class for our test set.  Recall how we used the
# predict function in the last project.
lda.pred = predict(lda.fit, test_set)
lda.class = lda.pred$class
# In R, we can use a boolean operator to determine accuracy.  Consider the next line of code.
match = lda.class == test_set$Type
# Determine the percent of accuracy using match.
match
q = 0
for(i in match)
{
if (i=="TRUE"){
q = q + 1
}
}
accuracy = q / length(match) * 100
accuracy
#
# Title: Glass Classification Project
#
# Author: Amanda Landi
#
# Sources:
# (1) where the data comes from: package mlbench, see documentation for details
#
# You need to install the following package
# You do not need to do so every time.  Only once.
# You may comment out the next two lines after you run them the first time.
# install.packages("mlbench")
# install.packages("MASS")
# However, you will need to run the next three lines of code
# Everytime you reopen the script.
library(mlbench)
library(MASS)
data("Glass")
# replace the underline with a function that allows you
# to see the structure of the data.
# Q: Describe the data in your report.
# RSC: str() function gets structure of data
str(Glass)
# Q: How large is your sample?
n = dim(Glass)
# Q: What percent of your data will become your training data? Why?
# Q: If we have too high a percentage, what do we need to worry about?
p = .9
# Please mimick what we did in the spam project when we extracted p% of the
# original data to creating a training data
percent = floor(n[1]*p)
t = sample(1:n[1], percent, replace = FALSE)
ind_train = t
test_set = Glass[-t,]
# Use the help page (or online documentation that you cite) to determine
# the appropriate use of the lda function
# Please choose only two of independent variables (the columns).
lda.fit = lda(Glass$Type ~ ., data = Glass[,c(1,2)], subset = t)
# the next line of code allows you to view your results
# Q: Discuss the prior probability results.
# Q: Discuss the meaning of Group Means.
# Q: What information do the coefficients give you?
lda.fit
# Create a graph.  You should use the help page to figure out how to
# include a meaningful title and axis labels.
plot(lda.fit, col = lda.fit$counts)
# We want to predict the class for our test set.  Recall how we used the
# predict function in the last project.
lda.pred = predict(lda.fit, test_set)
lda.class = lda.pred$class
# In R, we can use a boolean operator to determine accuracy.  Consider the next line of code.
match = lda.class == test_set$Type
# Determine the percent of accuracy using match.
match
q = 0
for(i in match)
{
if (i=="TRUE"){
q = q + 1
}
}
accuracy = q / length(match) * 100
accuracy
str(Glass)
n = dim(Glass)
percent = floor(n[1]*p)
t = sample(1:n[1], percent, replace = FALSE)
ind_train = t
test_set = Glass[-t,]
lda.fit = lda(Glass$Type ~ ., data = Glass[,c(1,2)], subset = t)
library(mlbench)
library(MASS)
data("Glass")
str(Glass)
n = dim(Glass)
p = .8
percent = floor(n[1]*p)
t = sample(1:n[1], percent, replace = FALSE)
ind_train = t
test_set = Glass[-t,]
lda.fit = lda(Glass$Type ~ ., data = Glass[,c(1,2)], subset = t)
lda.fit
lda.fit
mean(14, 14.3, 14.2, 15.1)
mean(14.0, 14.3, 14.2, 15.1)
mean(12.1, 13.6, 11.9, 11.2)
var(14.0, 14.3, 14.2, 15.1)
var(list(14.0, 14.3, 14.2, 15.1))
var(c(14.0, 14.3, 14.2, 15.1))
var(c(12.1, 13.6, 11.9, 11.2))
var(c(14.0, 14.3, 12.2, 15.1))
mean(14.0, 14.3, 12.2, 15.1)
mean(c(14.0, 14.3, 12.2, 15.1))
mean(c(12.1, 13.6, 11.9, 11.2))
H = list(1.2, .9, .7, 1.0, 1.7, 1.7, 1.1, .9, 1.7, 1.9, 1.3, 2.1, 1.6, 1.8, 1.4, 1.3, 1.9, 1.6, .8, 2, 1.7, 1.6, 2.3, 2.0)
qqnorm(H)
H = c(1.2, .9, .7, 1.0, 1.7, 1.7, 1.1, .9, 1.7, 1.9, 1.3, 2.1, 1.6, 1.8, 1.4, 1.3, 1.9, 1.6, .8, 2, 1.7, 1.6, 2.3, 2.0)
qqnorm(H)
qqline(H)
P = c(1.6, 1.5, 1.1, 2.1, 1.5, 1.3, 1.0, 2.6)
qqnorm(P)
qqline(P)
c(1.6, 1.8) + "P"
c(1.6, 1.8) + c("P")
c(1.6, 1.8) , c("P")
data.frame(c(1.8, "P"))
data.frame(c(1.8), c("P"))
c("H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H")
c("H")*24
HL = c("H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H", "H")
PL = c("P", "P", "P", "P", "P", "P", "P", "P")
nums = c(H, P)
nums
labs = c(HL, PL)
labs
x = data.frame(nums=nums, labs=labs)
x
b = boxplot(split(x$nums, x$labs), xlab="Quality", ylab="Extensibility")
mean(H)
mean(P)
qf(.975, df1=22, df2=19)
qf(.025, df1=22, df2=19)
qf(.05, df1=22, df2=19)
diffs = c(11, 164, 225, -9, 169, 442, -312, 69, 137, 157, -54, 350, 452, 540)
qqplot(diffs)
qplot(diffs)
qqnorm(diffs)
qqline(diffs)
# YES, it is plausible
mean(diffs)
var(diffs)
sqrt(var(diffs))
length(diffs)
sqrt(length(diffs))
2*pt(2.740, 13, lower=FALSE)
install.packages("mlbench")
library(mlbench)
data("BreastCancer")
glimpse(BreastCancer)
library(mlbench)
library(dplyr)
data("BreastCancer")
glimpse(BreastCancer)
BreastCancer
typeof(BreastCancer)
df = data.frame(BreastCancer)
df
length(BreastCancer)
length(BreastCancer[0])
length(BreastCancer[,0])
df[sample(nrow(df),3),]
df[sample(nrow(df),1),]
df[sample(nrow(df),7),]
sample_n(df, 10)
bc = BreastCancer
t_percent = 0.75
smp_size <- floor(0.75 *nrow(bc))
smp_size
bc = bc[bc$Bare.nuclei != "<NA>"]
install.packages("mlbench")
library(dplyr)
data("BreastCancer")
glimpse(BreastCancer)
bc = BreastCancer
bc = bc[bc$Bare.nuclei != "<NA>"]
class(bc)
typeof(bc)
summary(bc)
colnames(bc)
bc[-"Bare.nuclei"]
colnames(bc[ , !(names(bc) == "Bare.nuclei")])
variables = colnames(bc)
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
f
summary(bc)
variables = colnames(bc[ , !(names(bc) == "Class")])
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
f
colnames(bc)
train
smp_size <- floor(0.75 *nrow(bc))
set.seed(123)
train_ind <- sample(seq_len(nrow(bc)), size=smp_size)
train <- bc[train_ind, ]
test <- bc[-train_ind, ]
variables = colnames(bc[ , !(names(bc) == "Class")])
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
summary(bc)
variables = ["Cl.thickness", "Cell.size"]
variables = list("Cl.thickness", "Cell.size")
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
log_reg
summary(log_reg)
bc = BreastCancer
variables = colnames(bc[ , !(names(bc) == "Class")])
variables = list("Cl.thickness", "Cell.size")
bc = bc[ , !(names(bc) %in% variables)]
smp_size <- floor(0.75 *nrow(bc))
set.seed(123)
train_ind <- sample(seq_len(nrow(bc)), size=smp_size)
train <- bc[train_ind, ]
test <- bc[-train_ind, ]
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
pred = predict(model, test, type = "response").
pred[pred > threshold] = "malignant"
summary(bv)
summary(bc)
bc = BreastCancer
variables = colnames(bc[ , !(names(bc) == "Class")])
variables = list("Cl.thickness", "Cell.size")
bc = bc[ , (names(bc) %in% variables)]
smp_size <- floor(0.75 *nrow(bc))
set.seed(123)
train_ind <- sample(seq_len(nrow(bc)), size=smp_size)
train <- bc[train_ind, ]
test <- bc[-train_ind, ]
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
pred = predict(model, test, type = "response").
pred[pred > threshold] = "malignant"
pred = predict(model, test, type = "response")
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
variables = colnames(bc[ , !(names(bc) == "Class")])
variables = list("Class, Cl.thickness", "Cell.size")
bc = bc[ , (names(bc) %in% variables)]
smp_size <- floor(0.75 *nrow(bc))
set.seed(123)
train_ind <- sample(seq_len(nrow(bc)), size=smp_size)
variables = list("Class, Cl.thickness", "Cell.size")
bc = bc[ , (names(bc) %in% variables)]
variables = colnames(bc[ , !(names(bc) == "Class")])
variables = list("Class", "Cl.thickness", "Cell.size")
bc = BreastCancer
variables = colnames(bc[ , !(names(bc) == "Class")])
variables = list("Class", "Cl.thickness", "Cell.size")
bc = bc[ , (names(bc) %in% variables)]
smp_size <- floor(0.75 *nrow(bc))
set.seed(123)
train_ind <- sample(seq_len(nrow(bc)), size=smp_size)
train <- bc[train_ind, ]
test <- bc[-train_ind, ]
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
pred = predict(model, test, type = "response")
x = c(89, 177, 189, 354, 362, 442, 965)
y = c(.4, .6, .48, .66, .61, .69, .99)
x = c(89, 177, 189, 354, 362, 442, 965)
y = c(.4, .6, .48, .66, .61, .69, .99)
df = data.fram(x = x, y = y)
df = data.frame(x = x, y = y)
y = c(.4, .6, .48, .66, .61, .69, .99)
df = data.frame(x = x, y = y)
plot(df$x, df$y)
x = c(89, 177, 189, 354, 362, 442, 965)
y = c(.4, .6, .48, .66, .61, .69, .99)
df = data.frame(x = x, y = y)
plot(df$x, df$y)
linearMod <- lm(y ~ x, data=df)
linearMod
summary(linearMod)
linearMod[1]
linearMod[1][0]
linearMod[1][1]
linearMod[1][2]
linearMod$coefficients
linearMod$coefficients[1]
linearMod$coefficients[1]+1
b0 = linearMod$coefficients[1]
b1 = linearMod$coefficients[2]
b0
b1
typof(b0)
typeof(b0)
x = c(89, 177, 189, 354, 362, 442, 965)
summary(linearMod)
r2 = linearMod$r.squared
r2
r2 = summary(linearMod)$r.squared
r2
library(mlbench)
library(dplyr)
data("BostonHousing")
glimpse(BostonHousing)
glimpse(bh)
library(mlbench)
library(dplyr)
data("BostonHousing")
bh = BreastCancer
glimpse(bh)
library(mlbench)
library(dplyr)
data("BostonHousing")
bh = BostonHousing
glimpse(bh)
head(bh)
str(bh)
summary(bh)
summary(bh)
lmAge = lm(bh$crim ~ bh$age, bh)
summary(lmAge)
plot(lmAge)
plot(bh[,c("crim", "age")])
glimpse(bh)
summary(bh)
plot(bh[,c("crim", "tax")])
plot(bh[,c("crim", "zn")])
plot(bh[,c("crim", "indus")])
plot(bh[,c("crim", "ptratio")])
plot(bh[,c("crim", "b")])
plot(bh[,c("crim", "nox")])
train <- bc[train_ind, ]
test <- bc[-train_ind, ]
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
pred = predict(model, test, type = "response")
install.packages("mlbench")
library(mlbench)
library(dplyr)
data("BreastCancer")
glimpse(BreastCancer)
bc = BreastCancer
variables = colnames(bc[ , !(names(bc) == "Class")])
variables = list("Class", "Cl.thickness", "Cell.size")
bc = bc[ , (names(bc) %in% variables)]
smp_size <- floor(0.75 *nrow(bc))
set.seed(123)
train_ind <- sample(seq_len(nrow(bc)), size=smp_size)
train <- bc[train_ind, ]
test <- bc[-train_ind, ]
f = as.formula(paste("Class", "~", paste( variables , collapse = "+")))
log_reg = glm(f, family = binomial("logit"), data = train)
train
test
sand = as.factor(c("0","0","15","15","30","30","0","0","15","15","30","30","0","0","15","15","30","30"))
carbon = as.factor(c(rep("0",6),rep("0.25",6), rep("0.50",6)))
casting = c(61.0,63.0,67.0,69.0,65.0,74.0,69.0,69.0,69.0,74.0,74.0,72.0,67.0,69.0,69.0,74.0,74.0,74.0)
mold = c(34.0,16.0,36.0,19.0,28.0,17.0,49.0,48.0,43.0,29.0,31.0,24.0,55.0,60.0,45.0,43.0,22.0,48.0)
data = data.frame("sand" = sand, "carbon" = carbon, "casting" = casting, "mold" = mold)
av = aov(mold~sand+carbon+casting, data = data)
summary(av)
plot(summary(av))
y16 = read.table("clean/2016.csv")
setwd("~/Repos/performance-vs-pay")
y16 = read.table("clean/2016.csv")
y17 = read.table("clean/2017.csv")
summary(y16)
y16 = read.table("clean/2016.csv", sep = ",")
y17 = read.table("clean/2017.csv", sep = ",")
summary(y16)
y16 = read.table("clean/2016.csv", header = TRUE, sep = ",")
y17 = read.table("clean/2017.csv", header = TRUE, sep = ",")
summary(y16)
summary(y17)
plot(y16$Rating, y16$Compensation)
plot(y16$Rating, y16$Compensation)
plot(y17$Rating, y17$Compensation)
plot(y16$Rating, y16$BaseSalary)
plot(y17$Rating, y17$BaseSalary)
plot(y16$BaseSalary, y16$BaseSalary)
plot(y17$BaseSalary, y17$BaseSalary)
plot(y16$BaseSalary, y16$BaseSalary)
plot(y17$BaseSalary, y17$BaseSalary)
plot(y16$BaseSalary, y16$BaseSalary)
plot(y17$BaseSalary, y17$BaseSalary)
plot(y17$BaseSalary, y17$Goals)
plot(y17$Goals, y17$BaseSalary)
